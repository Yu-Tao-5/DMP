{
    "document_id": "D-2024-2714",
    "LinkTitle": "D-2024-2714",
    "file_name": "D-2024-2714.pdf",
    "file_path": "/Users/JADEPOTTER5/Downloads/DMP-MT/processed_data/pdfs_new/org_pdfs/D-2024-2714.pdf",
    "metadata": {
        "title": "Design of novel signal processing algorithms for the detection and reconstruction of gravitational waves emitted by core-collapse supernovae",
        "author": "N/A",
        "num_pages": 6
    },
    "content": {
        "full_text": "Design of novel signal processing algorithms for the detection and reconstruction of gravitational waves emitted\nDesign of novel signal processing algorithms for the detection and reconstruction of gravitational waves emitted\nby core-collapse supernovae\nby core-collapse supernovae\nA Data Management Plan created using DMPonline.be\nCreator: \nCreator: \nMilan Wils\nAffiliation: \nAffiliation: \nKU Leuven (KUL)\nFunder: \nFunder: \nFonds voor Wetenschappelijk Onderzoek - Research Foundation Flanders (FWO)\nTemplate: \nTemplate: \nFWO DMP (Flemish Standard DMP)\nGrant number / URL: \nGrant number / URL: \n11POK24N\nID: \nID: \n206759\nStart date: \nStart date: \n01-11-2023\nEnd date: \nEnd date: \n31-10-2027\nProject abstract:\nProject abstract:\nSince the first detection of gravitational waves (GWs) by the LIGO detectors of a binary black hole\nmerger in 2015, many more binary mergers have been detected. A new generation of GW detectors\nwill be build, whose improved sensitivity will allow the detection of core-collapse supernovae\n(CCSNe). Those detections can result in a better understanding of CCSNe, interaction between the\nfundamental forces of nature in extreme environments and astrophysical processes that depend on\nCCSNe such as nucleosynthesis. New algorithms are required because CCSNe waveforms are\nestimated through various simulations and are stochastic. The proposal aims to develop these new\nalgorithms by applying state-of-the art digital signal processing principles. The null-stream analysis\nwill be repurposed to detect GWs with low latency and minimal assumptions on the source while\ndeterministic section of the GW will be targetted by a template-based search. The maximal amount\nof information will be extracted from the waveform by exploiting the multi-messenger structure of\nCCSNe. A generalised sidelobe cancellation (GSC) structure will attempt to exploit the information in\nthe null-stream for noise reduction. An optimal wavelet transformation will be selected to improve\nthe reconstruction of the GW. Extensive simulations will be performed to investigate the maximal\nsource distance of the proposed algorithms and compare them to the state-of-the-art.\nLast modified: \nLast modified: \n29-04-2024\nCreated using DMPonline.be. Last modiﬁed 29 April 2024\n1 of 6\nDesign of novel signal processing algorithms for the detection and reconstruction of gravitational waves emitted\nDesign of novel signal processing algorithms for the detection and reconstruction of gravitational waves emitted\nby core-collapse supernovae\nby core-collapse supernovae\nFWO DMP (Flemish Standard DMP)\nFWO DMP (Flemish Standard DMP)\n1. Research Data Summary\n1. Research Data Summary\nList and describe all datasets or research materials that you plan to generate/collect or reuse during your research project. For each dataset or\nList and describe all datasets or research materials that you plan to generate/collect or reuse during your research project. For each dataset or\ndata type (observational, experimental etc.), provide a short name & description (sufficient for yourself to know what data it is about), indicate\ndata type (observational, experimental etc.), provide a short name & description (sufficient for yourself to know what data it is about), indicate\nwhether the data are newly generated/collected or reused, digital or physical, also indicate the type of the data (the kind of content), its\nwhether the data are newly generated/collected or reused, digital or physical, also indicate the type of the data (the kind of content), its\ntechnical format (file extension), and an estimate of the upper limit of the volume of the data.\ntechnical format (file extension), and an estimate of the upper limit of the volume of the data.\n \n \n \n \nOnly for digital data\nOnly for\ndigital\ndata \nOnly for\ndigital data \nOnly for\nphysical\ndata\nDataset Name\nDescription\nNew or\nreused\nDigital\nor\nPhysical\nDigital Data Type\nDigital\nData\nformat\nDigital data\nvolume\n(MB/GB/TB)\nPhysical\nvolume\nLVK O4\nGravitational wave strain data from the fourth\nobserving run, including calibration uncertainty\nReuse\nexisting\ndata\nDigital\nObservational\n.gwf, .h5,\n.txt\n< 50 TB\n \nET MDC\nMock Data Challenge for the Einstein Telescope\nGenerate\nnew data\nDigital\nSimulation data\n.gwf\n< 1 TB\n \ncWB\ncoherent WaveBurst\nReuse\nexisting\ndata\nDigital\nSoftware\n.C, .h\n< 100 MB\n \ncWB Analyses\ncoherent WaveBurst analysis setups and results\nGenerate\nnew data\nDigital\nCompiled/aggregated\ndata\n.root, .h5,\n.html,\n.png, .txt\n< 1 TB\n \nPython Scripts\nPackages that are developed, outreach demos,\ndata visualisation scripts etc.\nGenerate\nnew data\nDigital\nSoftware\n.py\n< 100 MB\n \nTensorlab\nMatlab/Python toolbox for tensor methods\nReuse\ndata\nDigital\nSoftware\n.m, .py\n< 100 MB\n \nSupernova\nwaveforms\nSimulated 2D and 3D gravitational wave\nsignatures of core-collapse supernovae\nReuse\ndata\nDigital\nSimulation data\n.csv, .txt,\n.dat, .h5\n< 100 MB\n \nET Supernova\nsearch pipeline\nUnmodelled search pipeline for the Einstein\nTelescope\nGenerate\nnew data\nDigital\nSoftware\n.cpp, .hpp,\n.py\n< 100 MB\n \nIf you reuse existing data, please specify the source, preferably by using a persistent identifier (e.g. DOI, Handle, URL etc.) per dataset or data\nIf you reuse existing data, please specify the source, preferably by using a persistent identifier (e.g. DOI, Handle, URL etc.) per dataset or data\ntype:\ntype:\nLVK O4 Data\nProprietary data of the LIGO-Virgo-KAGRA collaboration\nRelevant data will be made public on the Gravitational Wave Open Science Center (\nGWOSC\n) on 23/05/2026\ncWB\nLatest version is proprietary to the LIGO-Virgo-KAGRA collaboration\nCurrent public version (\nO3\n)\nTensorlab\nLatest version\nSupernova waveforms\nNot a uniform dataset but a compilation of 10-20 published waveforms in the literature. The compilation of this list is part of the\nresearch project.\nAre there any ethical issues concerning the creation and/or use of the data (e.g. experiments on humans or animals, dual use)? Describe these\nAre there any ethical issues concerning the creation and/or use of the data (e.g. experiments on humans or animals, dual use)? Describe these\nissues in the comment section. Please refer to specific datasets or data types when appropriate.\nissues in the comment section. Please refer to specific datasets or data types when appropriate.\nCreated using DMPonline.be. Last modiﬁed 29 April 2024\n2 of 6\nNo\nWill you process personal data? If so, briefly describe the kind of personal data you will use in the comment section. Please refer to specific\nWill you process personal data? If so, briefly describe the kind of personal data you will use in the comment section. Please refer to specific\ndatasets or data types when appropriate.\ndatasets or data types when appropriate.\nNo\nDoes your work have potential for commercial valorization (e.g. tech transfer, for example spin-offs, commercial exploitation, …)? If so,\nDoes your work have potential for commercial valorization (e.g. tech transfer, for example spin-offs, commercial exploitation, …)? If so,\nplease comment per dataset or data type where appropriate.\nplease comment per dataset or data type where appropriate.\nNo\nDo existing 3rd party agreements restrict exploitation or dissemination of the data you (re)use (e.g. Material/Data transfer agreements/ research\nDo existing 3rd party agreements restrict exploitation or dissemination of the data you (re)use (e.g. Material/Data transfer agreements/ research\ncollaboration agreements)? If so, please explain in the comment section to what data they relate and what restrictions are in place.\ncollaboration agreements)? If so, please explain in the comment section to what data they relate and what restrictions are in place.\nYes\nAny analyses results that are the result of a non-public version of cWB and/or non-public LVK data have to be approved by the LVK\nCollaboration prior to publication.\nAre there any other legal issues, such as intellectual property rights and ownership, to be managed related to the data you (re)use? If so, please\nAre there any other legal issues, such as intellectual property rights and ownership, to be managed related to the data you (re)use? If so, please\nexplain in the comment section to what data they relate and which restrictions will be asserted.\nexplain in the comment section to what data they relate and which restrictions will be asserted.\nNo\n2. Documentation and Metadata\n2. Documentation and Metadata\nClearly describe what approach will be followed to capture the accompanying information necessary to keep data understandable and usable,\nClearly describe what approach will be followed to capture the accompanying information necessary to keep data understandable and usable,\nfor yourself and others, now and in the future (e.g., in terms of documentation levels and types required, procedures used, Electronic Lab\nfor yourself and others, now and in the future (e.g., in terms of documentation levels and types required, procedures used, Electronic Lab\nNotebooks, README.txt files, Codebook.tsv etc. where this information is recorded).\nNotebooks, README.txt files, Codebook.tsv etc. where this information is recorded).\nThe generated datasets will be accompanied by a readme file that specifies assumptions made during its generation, as well as a link to the\ncode repository that was used to generate it. Standard data formats such as Gravitational-Wave Frames (.gwf), HDF5 and ROOT will be used\nto avoid ambiguity in the data specification.\nAll generated code will be documented through descriptions in comments. For large codebases such as the unmodeled search pipeline,\ndoxygen will be used to automatically generate documentation. All, packages will include installation instructions through a readme file.\nWill a metadata standard be used to make it easier to find and reuse the data? If so, please specify (where appropriate per dataset or data type)\nWill a metadata standard be used to make it easier to find and reuse the data? If so, please specify (where appropriate per dataset or data type)\nwhich metadata standard will be used. If not, please specify (where appropriate per dataset or data type) which metadata will be created to\nwhich metadata standard will be used. If not, please specify (where appropriate per dataset or data type) which metadata will be created to\nmake the data easier to find and reuse.\nmake the data easier to find and reuse.\nYes\nThe simulated data will store the metadata in the standardised header of .gwf files.\nCreated using DMPonline.be. Last modiﬁed 29 April 2024\n3 of 6\n3. Data storage & back-up during the research project\n3. Data storage & back-up during the research project\nWhere will the data be stored?\nWhere will the data be stored?\nAll generated code will be stored on Gitlab repositories. Depending on the use case and collaborators, this can any of the following servers:\ngitlab.et-gw.eu\ngit.ligo.org\ngitlab.kuleuven.be\ngitlab.esat.kuleuven.be\nThe generated dataset will be stored temporarily on collaboration computing clusters. After publication, the software to re-generate the data\nwill be made public. The software version and used settings will be provided as well to ensure reproduction capability.\nHow will the data be backed up?\nHow will the data be backed up?\nAll Gitlab repositories are follow an internal backup procedure (KU Leuven servers, IGWN Computing)\nIs there currently sufficient storage & backup capacity during the project? If yes, specify concisely.\nIs there currently sufficient storage & backup capacity during the project? If yes, specify concisely.\nIf no or insufficient storage or backup capacities are available, then explain how this will be taken care of.\nIf no or insufficient storage or backup capacities are available, then explain how this will be taken care of.\nYes\nThe size of the code repositories is negligible compared to common requirements on Gitlab servers.\nThe ET-MDC dataset and cWB analysis results are well within the accepted limit of the LVK CIT cluster limits\nHow will you ensure that the data are securely stored and not accessed or modified by unauthorized persons?\nHow will you ensure that the data are securely stored and not accessed or modified by unauthorized persons?\nCodes stored on the KU Leuven servers are protected through MFA.\nAll collaboration resources are protected through authorisation with the collaboration account. The LVK collaboration is also transitioning to\nMFA.\nWhat are the expected costs for data storage and backup during the research project? How will these costs be covered?\nWhat are the expected costs for data storage and backup during the research project? How will these costs be covered?\nBecause all code / generated data will be hosted on collaboration resources or KU Leuven Gitlab servers, there is no direct cost associated with\nits storage.\n4. Data preservation after the end of the research project\n4. Data preservation after the end of the research project\nWhich data will be retained for at least five years (or longer, in agreement with other retention policies that are applicable) after the end of the\nWhich data will be retained for at least five years (or longer, in agreement with other retention policies that are applicable) after the end of the\nproject? In case some data cannot be preserved, clearly state the reasons for this (e.g. legal or contractual restrictions, storage/budget issues,\nproject? In case some data cannot be preserved, clearly state the reasons for this (e.g. legal or contractual restrictions, storage/budget issues,\ninstitutional policies...).\ninstitutional policies...).\nAll data will be preserved for 10 years, according to the KU Leuven RDM policy. The exception is the Einstein Telescope Mock Data where\nonly the software and its configuration will be stored due to its large volume (1 TB) and reproducibility ( <1 day on a laptop).\nWhere will these data be archived (stored and curated for the long-term)?\nWhere will these data be archived (stored and curated for the long-term)?\nAll generated code will be stored on Gitlab repositories. Depending on the use case and collaborators, this can any of the following servers:\nCreated using DMPonline.be. Last modiﬁed 29 April 2024\n4 of 6\ngitlab.et-gw.eu\ngit.ligo.org\ngitlab.kuleuven.be\ngitlab.esat.kuleuven.be\nWhat are the expected costs for data preservation during the expected retention period? How will these costs be covered?\nWhat are the expected costs for data preservation during the expected retention period? How will these costs be covered?\nThere are no direct costs because all data will be stored on collaboration resources or a KU Leuven Gitlab server.\n5. Data sharing and reuse\n5. Data sharing and reuse\nWill the data (or part of the data) be made available for reuse after/during the project?  In the comment section please explain per dataset or\nWill the data (or part of the data) be made available for reuse after/during the project?  In the comment section please explain per dataset or\ndata type which data will be made available.\ndata type which data will be made available.\nNo (closed access)\nYes, in an Open Access repository\nAll code developped during this project will be made public as soon as it is stable and the first results have been published.\ncWB simulation results will not be made public because they are the result of proprietary data.\nThe configuration of the ET MDC will be made public.\nIf access is restricted, please specify who will be able to access the data and under what conditions.\nIf access is restricted, please specify who will be able to access the data and under what conditions.\nThe cWB analysis results will be accesible to all LVK members\nAre there any factors that restrict or prevent the sharing of (some of) the data (e.g. as defined in an agreement with a 3rd party, legal\nAre there any factors that restrict or prevent the sharing of (some of) the data (e.g. as defined in an agreement with a 3rd party, legal\nrestrictions)? Please explain in the comment section per dataset or data type where appropriate.\nrestrictions)? Please explain in the comment section per dataset or data type where appropriate.\nYes, Intellectual Property Rights\nAll products derived from non-public versions of LVK Collaboration code and / or data are subject to approval before they can be made public.\nWhere will the data be made available? If already known, please provide a repository per dataset or data type.\nWhere will the data be made available? If already known, please provide a repository per dataset or data type.\nAll code will be made available through their respective git repositories. The ET MDC data configuration will be available in the first\npublished paper.\nWhen will the data be made available?\nWhen will the data be made available?\nUpon publication of the results\nWhich data usage licenses are you going to provide? If none, please explain why.\nWhich data usage licenses are you going to provide? If none, please explain why.\nCreative Commons Attribution (CC-BY)\nDo you intend to add a PID/DOI/accession number to your dataset(s)? If already available, you have the option to provide it in the comment\nDo you intend to add a PID/DOI/accession number to your dataset(s)? If already available, you have the option to provide it in the comment\nsection.\nsection.\nCreated using DMPonline.be. Last modiﬁed 29 April 2024\n5 of 6\nNo\nThe initial publications will serve as the citable \nWhat are the expected costs for data sharing? How will these costs be covered?\nWhat are the expected costs for data sharing? How will these costs be covered?\nThere are no direct costs because all data will be stored on collaboration resources or a KU Leuven Gitlab server.\n6. Responsibilities\n6. Responsibilities\nWho will manage data documentation and metadata during the research project?\nWho will manage data documentation and metadata during the research project?\nMilan Wils \nWho will manage data storage and backup during the research project?\nWho will manage data storage and backup during the research project?\nMilan Wils \nWho will manage data preservation and sharing?\nWho will manage data preservation and sharing?\nMilan Wils \nWho will update and implement this DMP?\nWho will update and implement this DMP?\nMilan Wils \nCreated using DMPonline.be. Last modiﬁed 29 April 2024\n6 of 6"
    },
    "clean_full_text": "Design of novel signal processing algorithms for the detection and reconstruction of gravitational waves emitted Design of novel signal processing algorithms for the detection and reconstruction of gravitational waves emitted by core-collapse supernovae by core-collapse supernovae A Data Management Plan created using DMPonline.be Creator: Creator: Milan Wils Affiliation: Affiliation: KU Leuven (KUL) Funder: Funder: Fonds voor Wetenschappelijk Onderzoek - Research Foundation Flanders (FWO) Template: Template: FWO DMP (Flemish Standard DMP) Grant number / URL: Grant number / URL: 11POK24N ID: ID: 206759 Start date: Start date: 01-11-2023 End date: End date: 31-10-2027 Project abstract: Project abstract: Since the first detection of gravitational waves (GWs) by the LIGO detectors of a binary black hole merger in 2015, many more binary mergers have been detected. A new generation of GW detectors will be build, whose improved sensitivity will allow the detection of core-collapse supernovae (CCSNe). Those detections can result in a better understanding of CCSNe, interaction between the fundamental forces of nature in extreme environments and astrophysical processes that depend on CCSNe such as nucleosynthesis. New algorithms are required because CCSNe waveforms are estimated through various simulations and are stochastic. The proposal aims to develop these new algorithms by applying state-of-the art digital signal processing principles. The null-stream analysis will be repurposed to detect GWs with low latency and minimal assumptions on the source while deterministic section of the GW will be targetted by a template-based search. The maximal amount of information will be extracted from the waveform by exploiting the multi-messenger structure of CCSNe. A generalised sidelobe cancellation (GSC) structure will attempt to exploit the information in the null-stream for noise reduction. An optimal wavelet transformation will be selected to improve the reconstruction of the GW. Extensive simulations will be performed to investigate the maximal source distance of the proposed algorithms and compare them to the state-of-the-art. Last modified: Last modified: 29-04-2024 Created using DMPonline.be. Last modiﬁed 29 April 2024 1 of 6 Design of novel signal processing algorithms for the detection and reconstruction of gravitational waves emitted Design of novel signal processing algorithms for the detection and reconstruction of gravitational waves emitted by core-collapse supernovae by core-collapse supernovae FWO DMP (Flemish Standard DMP) FWO DMP (Flemish Standard DMP) 1. Research Data Summary 1. Research Data Summary List and describe all datasets or research materials that you plan to generate/collect or reuse during your research project. For each dataset or List and describe all datasets or research materials that you plan to generate/collect or reuse during your research project. For each dataset or data type (observational, experimental etc.), provide a short name & description (sufficient for yourself to know what data it is about), indicate data type (observational, experimental etc.), provide a short name & description (sufficient for yourself to know what data it is about), indicate whether the data are newly generated/collected or reused, digital or physical, also indicate the type of the data (the kind of content), its whether the data are newly generated/collected or reused, digital or physical, also indicate the type of the data (the kind of content), its technical format (file extension), and an estimate of the upper limit of the volume of the data. technical format (file extension), and an estimate of the upper limit of the volume of the data. Only for digital data Only for digital data Only for digital data Only for physical data Dataset Name Description New or reused Digital or Physical Digital Data Type Digital Data format Digital data volume (MB/GB/TB) Physical volume LVK O4 Gravitational wave strain data from the fourth observing run, including calibration uncertainty Reuse existing data Digital Observational .gwf, .h5, .txt < 50 TB ET MDC Mock Data Challenge for the Einstein Telescope Generate new data Digital Simulation data .gwf < 1 TB cWB coherent WaveBurst Reuse existing data Digital Software .C, .h < 100 MB cWB Analyses coherent WaveBurst analysis setups and results Generate new data Digital Compiled/aggregated data .root, .h5, .html, .png, .txt < 1 TB Python Scripts Packages that are developed, outreach demos, data visualisation scripts etc. Generate new data Digital Software .py < 100 MB Tensorlab Matlab/Python toolbox for tensor methods Reuse data Digital Software .m, .py < 100 MB Supernova waveforms Simulated 2D and 3D gravitational wave signatures of core-collapse supernovae Reuse data Digital Simulation data .csv, .txt, .dat, .h5 < 100 MB ET Supernova search pipeline Unmodelled search pipeline for the Einstein Telescope Generate new data Digital Software .cpp, .hpp, .py < 100 MB If you reuse existing data, please specify the source, preferably by using a persistent identifier (e.g. DOI, Handle, URL etc.) per dataset or data If you reuse existing data, please specify the source, preferably by using a persistent identifier (e.g. DOI, Handle, URL etc.) per dataset or data type: type: LVK O4 Data Proprietary data of the LIGO-Virgo-KAGRA collaboration Relevant data will be made public on the Gravitational Wave Open Science Center ( GWOSC ) on 23/05/2026 cWB Latest version is proprietary to the LIGO-Virgo-KAGRA collaboration Current public version ( O3 ) Tensorlab Latest version Supernova waveforms Not a uniform dataset but a compilation of 10-20 published waveforms in the literature. The compilation of this list is part of the research project. Are there any ethical issues concerning the creation and/or use of the data (e.g. experiments on humans or animals, dual use)? Describe these Are there any ethical issues concerning the creation and/or use of the data (e.g. experiments on humans or animals, dual use)? Describe these issues in the comment section. Please refer to specific datasets or data types when appropriate. issues in the comment section. Please refer to specific datasets or data types when appropriate. Created using DMPonline.be. Last modiﬁed 29 April 2024 2 of 6 No Will you process personal data? If so, briefly describe the kind of personal data you will use in the comment section. Please refer to specific Will you process personal data? If so, briefly describe the kind of personal data you will use in the comment section. Please refer to specific datasets or data types when appropriate. datasets or data types when appropriate. No Does your work have potential for commercial valorization (e.g. tech transfer, for example spin-offs, commercial exploitation, …)? If so, Does your work have potential for commercial valorization (e.g. tech transfer, for example spin-offs, commercial exploitation, …)? If so, please comment per dataset or data type where appropriate. please comment per dataset or data type where appropriate. No Do existing 3rd party agreements restrict exploitation or dissemination of the data you (re)use (e.g. Material/Data transfer agreements/ research Do existing 3rd party agreements restrict exploitation or dissemination of the data you (re)use (e.g. Material/Data transfer agreements/ research collaboration agreements)? If so, please explain in the comment section to what data they relate and what restrictions are in place. collaboration agreements)? If so, please explain in the comment section to what data they relate and what restrictions are in place. Yes Any analyses results that are the result of a non-public version of cWB and/or non-public LVK data have to be approved by the LVK Collaboration prior to publication. Are there any other legal issues, such as intellectual property rights and ownership, to be managed related to the data you (re)use? If so, please Are there any other legal issues, such as intellectual property rights and ownership, to be managed related to the data you (re)use? If so, please explain in the comment section to what data they relate and which restrictions will be asserted. explain in the comment section to what data they relate and which restrictions will be asserted. No 2. Documentation and Metadata 2. Documentation and Metadata Clearly describe what approach will be followed to capture the accompanying information necessary to keep data understandable and usable, Clearly describe what approach will be followed to capture the accompanying information necessary to keep data understandable and usable, for yourself and others, now and in the future (e.g., in terms of documentation levels and types required, procedures used, Electronic Lab for yourself and others, now and in the future (e.g., in terms of documentation levels and types required, procedures used, Electronic Lab Notebooks, README.txt files, Codebook.tsv etc. where this information is recorded). Notebooks, README.txt files, Codebook.tsv etc. where this information is recorded). The generated datasets will be accompanied by a readme file that specifies assumptions made during its generation, as well as a link to the code repository that was used to generate it. Standard data formats such as Gravitational-Wave Frames (.gwf), HDF5 and ROOT will be used to avoid ambiguity in the data specification. All generated code will be documented through descriptions in comments. For large codebases such as the unmodeled search pipeline, doxygen will be used to automatically generate documentation. All, packages will include installation instructions through a readme file. Will a metadata standard be used to make it easier to find and reuse the data? If so, please specify (where appropriate per dataset or data type) Will a metadata standard be used to make it easier to find and reuse the data? If so, please specify (where appropriate per dataset or data type) which metadata standard will be used. If not, please specify (where appropriate per dataset or data type) which metadata will be created to which metadata standard will be used. If not, please specify (where appropriate per dataset or data type) which metadata will be created to make the data easier to find and reuse. make the data easier to find and reuse. Yes The simulated data will store the metadata in the standardised header of .gwf files. Created using DMPonline.be. Last modiﬁed 29 April 2024 3 of 6 3. Data storage & back-up during the research project 3. Data storage & back-up during the research project Where will the data be stored? Where will the data be stored? All generated code will be stored on Gitlab repositories. Depending on the use case and collaborators, this can any of the following servers: gitlab.et-gw.eu git.ligo.org gitlab.kuleuven.be gitlab.esat.kuleuven.be The generated dataset will be stored temporarily on collaboration computing clusters. After publication, the software to re-generate the data will be made public. The software version and used settings will be provided as well to ensure reproduction capability. How will the data be backed up? How will the data be backed up? All Gitlab repositories are follow an internal backup procedure (KU Leuven servers, IGWN Computing) Is there currently sufficient storage & backup capacity during the project? If yes, specify concisely. Is there currently sufficient storage & backup capacity during the project? If yes, specify concisely. If no or insufficient storage or backup capacities are available, then explain how this will be taken care of. If no or insufficient storage or backup capacities are available, then explain how this will be taken care of. Yes The size of the code repositories is negligible compared to common requirements on Gitlab servers. The ET-MDC dataset and cWB analysis results are well within the accepted limit of the LVK CIT cluster limits How will you ensure that the data are securely stored and not accessed or modified by unauthorized persons? How will you ensure that the data are securely stored and not accessed or modified by unauthorized persons? Codes stored on the KU Leuven servers are protected through MFA. All collaboration resources are protected through authorisation with the collaboration account. The LVK collaboration is also transitioning to MFA. What are the expected costs for data storage and backup during the research project? How will these costs be covered? What are the expected costs for data storage and backup during the research project? How will these costs be covered? Because all code / generated data will be hosted on collaboration resources or KU Leuven Gitlab servers, there is no direct cost associated with its storage. 4. Data preservation after the end of the research project 4. Data preservation after the end of the research project Which data will be retained for at least five years (or longer, in agreement with other retention policies that are applicable) after the end of the Which data will be retained for at least five years (or longer, in agreement with other retention policies that are applicable) after the end of the project? In case some data cannot be preserved, clearly state the reasons for this (e.g. legal or contractual restrictions, storage/budget issues, project? In case some data cannot be preserved, clearly state the reasons for this (e.g. legal or contractual restrictions, storage/budget issues, institutional policies...). institutional policies...). All data will be preserved for 10 years, according to the KU Leuven RDM policy. The exception is the Einstein Telescope Mock Data where only the software and its configuration will be stored due to its large volume (1 TB) and reproducibility ( <1 day on a laptop). Where will these data be archived (stored and curated for the long-term)? Where will these data be archived (stored and curated for the long-term)? All generated code will be stored on Gitlab repositories. Depending on the use case and collaborators, this can any of the following servers: Created using DMPonline.be. Last modiﬁed 29 April 2024 4 of 6 gitlab.et-gw.eu git.ligo.org gitlab.kuleuven.be gitlab.esat.kuleuven.be What are the expected costs for data preservation during the expected retention period? How will these costs be covered? What are the expected costs for data preservation during the expected retention period? How will these costs be covered? There are no direct costs because all data will be stored on collaboration resources or a KU Leuven Gitlab server. 5. Data sharing and reuse 5. Data sharing and reuse Will the data (or part of the data) be made available for reuse after/during the project? In the comment section please explain per dataset or Will the data (or part of the data) be made available for reuse after/during the project? In the comment section please explain per dataset or data type which data will be made available. data type which data will be made available. No (closed access) Yes, in an Open Access repository All code developped during this project will be made public as soon as it is stable and the first results have been published. cWB simulation results will not be made public because they are the result of proprietary data. The configuration of the ET MDC will be made public. If access is restricted, please specify who will be able to access the data and under what conditions. If access is restricted, please specify who will be able to access the data and under what conditions. The cWB analysis results will be accesible to all LVK members Are there any factors that restrict or prevent the sharing of (some of) the data (e.g. as defined in an agreement with a 3rd party, legal Are there any factors that restrict or prevent the sharing of (some of) the data (e.g. as defined in an agreement with a 3rd party, legal restrictions)? Please explain in the comment section per dataset or data type where appropriate. restrictions)? Please explain in the comment section per dataset or data type where appropriate. Yes, Intellectual Property Rights All products derived from non-public versions of LVK Collaboration code and / or data are subject to approval before they can be made public. Where will the data be made available? If already known, please provide a repository per dataset or data type. Where will the data be made available? If already known, please provide a repository per dataset or data type. All code will be made available through their respective git repositories. The ET MDC data configuration will be available in the first published paper. When will the data be made available? When will the data be made available? Upon publication of the results Which data usage licenses are you going to provide? If none, please explain why. Which data usage licenses are you going to provide? If none, please explain why. Creative Commons Attribution (CC-BY) Do you intend to add a PID/DOI/accession number to your dataset(s)? If already available, you have the option to provide it in the comment Do you intend to add a PID/DOI/accession number to your dataset(s)? If already available, you have the option to provide it in the comment section. section. Created using DMPonline.be. Last modiﬁed 29 April 2024 5 of 6 No The initial publications will serve as the citable What are the expected costs for data sharing? How will these costs be covered? What are the expected costs for data sharing? How will these costs be covered? There are no direct costs because all data will be stored on collaboration resources or a KU Leuven Gitlab server. 6. Responsibilities 6. Responsibilities Who will manage data documentation and metadata during the research project? Who will manage data documentation and metadata during the research project? Milan Wils Who will manage data storage and backup during the research project? Who will manage data storage and backup during the research project? Milan Wils Who will manage data preservation and sharing? Who will manage data preservation and sharing? Milan Wils Who will update and implement this DMP? Who will update and implement this DMP? Milan Wils Created using DMPonline.be. Last modiﬁed 29 April 2024 6 of 6"
}