{
    "document_id": "D-2023-1976",
    "LinkTitle": "D-2023-1976",
    "file_name": "D-2023-1976.pdf",
    "file_path": "/Users/JADEPOTTER5/Downloads/DMP-MT/processed_data/pdfs_new/org_pdfs/D-2023-1976.pdf",
    "metadata": {
        "title": "Beyond optimization: an algorithmic framework for structured nonmonotone inclusions",
        "author": "N/A",
        "num_pages": 5
    },
    "content": {
        "full_text": "Beyond optimization: an algorithmic framework for structured nonmonotone inclusions\nA Data Management Plan created using DMPonline.be\nCreator: \nPieter Pas\nAffiliation: \nKU Leuven (KUL)\nFunder: \nFonds voor Wetenschappelijk Onderzoek - Research Foundation Flanders (FWO)\nTemplate: \nFWO DMP (Flemish Standard DMP)\nGrant number / URL: \n11M9523N\nID: \n199079\nStart date: \n01-11-2022\nEnd date: \n01-11-2026\nProject abstract:\nThis project aims to generalize the mathematics of monotone inclusions (generalized equations) to\nthe nonmonotone case.\nOne type of problem that gives rise to monotone inclusions is the minimization of a convex function,\nthrough the first-order optimality conditions. When minimizing nonconvex functions, however,\nmonotonicity of the corresponding inclusion is lost, which makes finding solutions much more\nchallenging. Going beyond minimization problems, many other interesting problems in science and\nengineering can be posed as the solution of general nonmonotone inclusions. One important example\nis bilevel optimization, a type of problem that arises in adversarial learning, generative adversarial\nnetworks, distributionally robust optimization, risk-averse control, game theory, and so on.\nThe mathematical foundation for solving nonmonotone inclusions is rather limited, and offers no\ntractable general algorithms either. This project aims to change that by developing an algorithmic\nframework for such problems. First, nonconvex optimization is investigated as a special case of the\nclass of nonmonotone inclusions, which is then generalized, implemented as an accessible software\npackage for nonconvex optimization and nonmonotone inclusions, and applied to common problems\nin control theory, game theory and machine learning.\nLast modified: \n29-04-2023\nCreated using DMPonline.be. Last modiﬁed 29 April 2023\n1 of 5\nBeyond optimization: an algorithmic framework for structured nonmonotone inclusions\nFWO DMP (Flemish Standard DMP)\n1. Research Data Summary\nList and describe all datasets or research materials that you plan to generate/collect or reuse during your research project. For each dataset or data type (observational,\nexperimental etc.), provide a short name & description (sufficient for yourself to know what data it is about), indicate whether the data are newly generated/collected or reused,\ndigital or physical, also indicate the type of the data (the kind of content), its technical format (file extension), and an estimate of the upper limit of the volume of the data.\nDataset\nName\nDescription\nNew or reused\nDigital or\nPhysical\nDigital Data Type\nDigital Data format\nDigital data\nvolume\n(MB/GB/TB)\n \n \nPlease choose from the following options:\nGenerate new data\nReuse existing data\nPlease choose\nfrom the\nfollowing\noptions:\nDigital\nPhysical\nPlease choose from\nthe following\noptions:\nObservational\nExperimental\nCompiled/\naggregated\ndata\nSimulation\ndata\nSoftware\nOther\nNA\nPlease choose from the following\noptions:\n.por, .xml, .tab, .cvs,.pdf,\n.txt, .rtf, .dwg, .gml, …\nNA\nPlease choose\nfrom the\nfollowing\noptions:\n<100MB\n<1GB\n<100GB\n<1TB\n<5TB\n<10TB\n<50TB\n>50TB\nNA\nPrimary\nsoftware\n(WP 1 and 3)\nSource code for the software developed as\npart of the PhD project.\nGenerate new data\nDigital\nSoftware\nPlain-text C++, Python, Julia, C,\nMatlab, Fortran source code\n(.cpp, .hpp, .py, .jl, .c, .m, .f, .f90),\nincluding supporting build recipes\n(.cmake), scripts (.sh) and\nMarkDown/HTML documentation\n(.md, .html, .css).\n<100MB\nSoftware\nfrom\nprevious\nprojects\nSource code of the software, numerical\nsolvers and utilities developed by other\n(past) members of the research group.\nReuse existing data\nDigital\nSoftware\nIdem.\n<100MB\nBenchmark\nproblem\ndescriptions\nand driver\ncode (WP 4)\nCasADi and PyTorch code for building the\noptimization and inclusion problems that will\nbe used to verify and benchmark the\nnumerical solvers.\nGenerate new data\nDigital\nSoftware\nPlain-text C++ and Python source\ncode (.cpp, .hpp, .py), including\nsupporting build recipes (.cmake,\nMakefile), scripts (.sh), and data\nfiles (.csv, .npy) with matrices\nused in the problems.\n<100MB\nBenchmark\nproblem\ncollections\nCollections of even more optimization and\ninclusion problems to benchmark the\nnumerical solvers.\nReuse existing data\nDigital\nSoftware\nPlain-text Fortran source code\n(.f), supporting build recipes (.sh),\nand problem description files\n(.sif).\n<10GB\nBenchmark\nresults (WP\n4)\nPerformance statistics (run time, number of\nevaluations, suboptimality, constraint\nviolation ...) of the numerical solvers\ndeveloped as part of the project, as well as\nthe statistics for similar third-party solvers.\nGenerate new data\nDigital\nSimulation data\nStructured records in YAML or\nJSON format, as well as comma-\nseparated numerical data (CSV).\n<1GB\nBenchmark\nsolutions\n(WP 4)\nNumerical solutions produced by the\ndifferent solvers for later comparison,\nverification, and visualization.\nGenerate new data\nDigital\nSimulation data\nPlain-text or binary numerical\ndata files (.csv, .npy, .mat, .dat).\n<1GB\nIf you reuse existing data, please specify the source, preferably by using a persistent identifier (e.g. DOI, Handle, URL etc.) per dataset or data type:\nSoftware developed as part of previous projects:\nhttps://github.com/kul-optec/QPALM\nhttps://github.com/kul-optec/QPALM.jl\nhttps://github.com/kul-optec/superscs\nBenchmark problem collections:\nGould, N.I.M., Orban, D. & Toint, P.L. CUTEst: a Constrained and Unconstrained Testing Environment with safe threads for mathematical optimization. \nComput Optim Appl\n \n60\n, 545–557\n(2015). \nhttps://doi.org/10.1007/s10589-014-9687-3\nAre there any ethical issues concerning the creation and/or use of the data (e.g. experiments on humans or animals, dual use)? Describe these issues in the comment section.\nPlease refer to specific datasets or data types when appropriate.\nNo\nWill you process personal data? If so, briefly describe the kind of personal data you will use in the comment section. Please refer to specific datasets or data types when appropriate.\nNo\nDoes your work have potential for commercial valorization (e.g. tech transfer, for example spin-offs, commercial exploitation, …)? If so, please comment per dataset or data type\nCreated using DMPonline.be. Last modiﬁed 29 April 2023\n2 of 5\nwhere appropriate.\nNo\nDo existing 3rd party agreements restrict exploitation or dissemination of the data you (re)use (e.g. Material/Data transfer agreements/ research collaboration agreements)? If so,\nplease explain in the comment section to what data they relate and what restrictions are in place.\nNo\nAre there any other legal issues, such as intellectual property rights and ownership, to be managed related to the data you (re)use? If so, please explain in the comment section to\nwhat data they relate and which restrictions will be asserted.\nNo\n2. Documentation and Metadata\nClearly describe what approach will be followed to capture the accompanying information necessary to keep data understandable and usable, for yourself and others, now and in\nthe future (e.g., in terms of documentation levels and types required, procedures used, Electronic Lab Notebooks, README.txt files, Codebook.tsv etc. where this information is\nrecorded).\nThe software is accompanied by extensive documentation, installation instructions and usage examples. The API reference is automatically generated using tools like Doxygen and Sphinx, based\non the docstrings and comments present in the source code.\nBoth the documentation and the API reference are available as HTML files and are hosted (either publicly or internally) for easy access.\nAll code is managed using the Git version control system, with clear commit messages, and tags are used to be able to easily switch back to a previous version (e.g. a specific release, or a\nversion corresponding to a publication).\nThe numerical experiments include not only MarkDown or README files describing each experiment and benchmark problem, but also scripts and/or makefiles to easily reproduce the data, as\nwell as scripts to visualize and analyze the existing data.\nWill a metadata standard be used to make it easier to find and reuse the data? If so, please specify (where appropriate per dataset or data type) which metadata standard will be\nused. If not, please specify (where appropriate per dataset or data type) which metadata will be created to make the data easier to find and reuse.\nYes\nBenchmark results will be managed using the \ndman\n Data Management package to keep structured data about the simulation together with the results. \nFor simulations of specific problems of interest (rather than benchmark collections), a system such as EngMeta could be considered.\nThe software is published on GitHub, with appropriate tags, and made accessible through Zenodo or Papers with Code, which include metadata about the software, and link to the corresponding\npublications on arXiv. The GitHub repositories also contain a CITATION.cff file to link back to the publications.\n3. Data storage & back-up during the research project\nWhere will the data be stored?\nDuring active use or development, the data will be stored on the researcher's workstation and/or laptop.\nAdditionally, a time-stamped version of the data will be stored on the STADIUS DATASET Server, which is our research unit's central storage server.\nSoftware will not only be kept locally, but also on the KU Leuven GitLab instance (for private projects) and on GitHub (for public/open-source projects) to allow for efficient collaboration.\nHow will the data be backed up?\nLocal copies of the Git repositories are stored on an ESAT workstation which is backed up regularly and automatically.\nAll data on the STADIUS DATASET Server are backed up daily and replicated to an off-site storage\nsystem housed in the ICTS data center.\nIs there currently sufficient storage & backup capacity during the project? If yes, specify concisely.\nIf no or insufficient storage or backup capacities are available, then explain how this will be taken care of.\nYes\nThe volume of data is low, storage and backup capacity are not an issue. The STADIUS DATASET Server has a total capacity of 14.88 TB. The capacity of the dataset server is\nmonitored daily by the ESAT system admins.\nHow will you ensure that the data are securely stored and not accessed or modified by unauthorized persons?\nThe private repositories on GitLab are only accessible to authorized users, verified through the central KU Leuven login system, which requires two-factor authentication.\nThe public GitHub repositories are managed by members of the research group only, and require two-factor authentication. Releases to software repositories are automated, and the API tokens\nnecessary to upload new releases are accessible only to the continuous integration workflows that can be triggered by authorized maintainers only.\nCreated using DMPonline.be. Last modiﬁed 29 April 2023\n3 of 5\nWhat are the expected costs for data storage and backup during the research project? How will these costs be covered?\nData storage and back up will be performed on the STADIUS DATASET Server and the ESAT/KU Leuven GitLab instance in pre-existing storage facilities of the Department of Electrical\nEngineering (ESAT). The data volume for this project is small (<14GB), and so are the expected costs. For small volumes like this, the costs are covered by the storage provider.\n4. Data preservation after the end of the research project\nWhich data will be retained for at least five years (or longer, in agreement with other retention policies that are applicable) after the end of the project? In case some data cannot be\npreserved, clearly state the reasons for this (e.g. legal or contractual restrictions, storage/budget issues, institutional policies...).\nAll research data (including software and numerical results used in publications) will be preserved for 10 years, according to the KU Leuven RDM policy.\nWhere will these data be archived (stored and curated for the long-term)?\nThe data will be stored on the university's central servers (with automatic back-up procedures).\nThe software itself will be additionally preserved indefinitely in public repositories on GitHub, PyPI and possibly others.\nWhat are the expected costs for data preservation during the expected retention period? How will these costs be covered?\nFree of charge for small datasets.\n5. Data sharing and reuse\nWill the data (or part of the data) be made available for reuse after/during the project?  In the comment section please explain per dataset or data type which data will be made\navailable.\nYes, in an Open Access repository\nYes, in a restricted access repository (after approval, institutional access only, …)\nSoftware and numerical results used in publications will be made available in public GitHub repositories.\nBefore publication, development of the software and preliminary experiments will be available in restricted GitLab repositories requiring intitutional access and/or personal authorization.\nIf access is restricted, please specify who will be able to access the data and under what conditions.\nRestricted GitLab repositories will be accessible to the supervisor and other members of the research group, as well as possible collaborators from other research groups.\nThe access of the data on the STADIUS DATASET Server is regulated by an access control list (ACL)\nthat grants:\n- read-write access to the project owner and the FWO fellow\n- read-only access to specific users, such as other members of our research group\nThe ACL is managed by the project owner (Panagiotis Patrinos).\nAre there any factors that restrict or prevent the sharing of (some of) the data (e.g. as defined in an agreement with a 3rd party, legal restrictions)? Please explain in the comment\nsection per dataset or data type where appropriate.\nNo\nWhere will the data be made available? If already known, please provide a repository per dataset or data type.\nhttps://github.com/kul-optec/alpaqa\n (Software)\nhttps://github.com/kul-optec/panoc-gauss-newton-ifac-experiments\n (Numerical simulations)\nhttps://github.com/kul-optec/pantr-cdc2023-experiments\n (Numerical simulations)\nWhen will the data be made available?\nUpon publication of research results.\nWhich data usage licenses are you going to provide? If none, please explain why.\nSoftware will be released under a copyleft license such as the GNU Lesser General Public License. Benchmarks and examples will be made available under a permissive MIT license.\nDo you intend to add a PID/DOI/accession number to your dataset(s)? If already available, you have the option to provide it in the comment section.\nYes\nSoftware will be made available through Zenodo or Papers with Code.\nCreated using DMPonline.be. Last modiﬁed 29 April 2023\n4 of 5\nWhat are the expected costs for data sharing? How will these costs be covered?\nGitHub hosting is free for open-source repositories. Sharing code through the KU Leuven GitLab instance is covered by the university.\n6. Responsibilities\nWho will manage data documentation and metadata during the research project?\nThe data documentation and metadata will be managed by the FWO fellow, Pieter Pas. \nWho will manage data storage and backup during the research project?\nThe FWO fellow, Pieter Pas will be responsible for data storage. The system administrators and data manager of the research division are responsible for the back up during and after the project. \nWho will manage data preservation and sharing?\nThe FWO Fellow, Pieter Pas, will be responsible for ensuring data preservation and reuse. \nWho will update and implement this DMP?\nThe project owner (Panagiotis Patrinos) bears the end responsibility of updating and implementing this DMP. \nCreated using DMPonline.be. Last modiﬁed 29 April 2023\n5 of 5"
    },
    "clean_full_text": "Beyond optimization: an algorithmic framework for structured nonmonotone inclusions A Data Management Plan created using DMPonline.be Creator: Pieter Pas Affiliation: KU Leuven (KUL) Funder: Fonds voor Wetenschappelijk Onderzoek - Research Foundation Flanders (FWO) Template: FWO DMP (Flemish Standard DMP) Grant number / URL: 11M9523N ID: 199079 Start date: 01-11-2022 End date: 01-11-2026 Project abstract: This project aims to generalize the mathematics of monotone inclusions (generalized equations) to the nonmonotone case. One type of problem that gives rise to monotone inclusions is the minimization of a convex function, through the first-order optimality conditions. When minimizing nonconvex functions, however, monotonicity of the corresponding inclusion is lost, which makes finding solutions much more challenging. Going beyond minimization problems, many other interesting problems in science and engineering can be posed as the solution of general nonmonotone inclusions. One important example is bilevel optimization, a type of problem that arises in adversarial learning, generative adversarial networks, distributionally robust optimization, risk-averse control, game theory, and so on. The mathematical foundation for solving nonmonotone inclusions is rather limited, and offers no tractable general algorithms either. This project aims to change that by developing an algorithmic framework for such problems. First, nonconvex optimization is investigated as a special case of the class of nonmonotone inclusions, which is then generalized, implemented as an accessible software package for nonconvex optimization and nonmonotone inclusions, and applied to common problems in control theory, game theory and machine learning. Last modified: 29-04-2023 Created using DMPonline.be. Last modiﬁed 29 April 2023 1 of 5 Beyond optimization: an algorithmic framework for structured nonmonotone inclusions FWO DMP (Flemish Standard DMP) 1. Research Data Summary List and describe all datasets or research materials that you plan to generate/collect or reuse during your research project. For each dataset or data type (observational, experimental etc.), provide a short name & description (sufficient for yourself to know what data it is about), indicate whether the data are newly generated/collected or reused, digital or physical, also indicate the type of the data (the kind of content), its technical format (file extension), and an estimate of the upper limit of the volume of the data. Dataset Name Description New or reused Digital or Physical Digital Data Type Digital Data format Digital data volume (MB/GB/TB) Please choose from the following options: Generate new data Reuse existing data Please choose from the following options: Digital Physical Please choose from the following options: Observational Experimental Compiled/ aggregated data Simulation data Software Other NA Please choose from the following options: .por, .xml, .tab, .cvs,.pdf, .txt, .rtf, .dwg, .gml, … NA Please choose from the following options: <100MB <1GB <100GB <1TB <5TB <10TB <50TB >50TB NA Primary software (WP 1 and 3) Source code for the software developed as part of the PhD project. Generate new data Digital Software Plain-text C++, Python, Julia, C, Matlab, Fortran source code (.cpp, .hpp, .py, .jl, .c, .m, .f, .f90), including supporting build recipes (.cmake), scripts (.sh) and MarkDown/HTML documentation (.md, .html, .css). <100MB Software from previous projects Source code of the software, numerical solvers and utilities developed by other (past) members of the research group. Reuse existing data Digital Software Idem. <100MB Benchmark problem descriptions and driver code (WP 4) CasADi and PyTorch code for building the optimization and inclusion problems that will be used to verify and benchmark the numerical solvers. Generate new data Digital Software Plain-text C++ and Python source code (.cpp, .hpp, .py), including supporting build recipes (.cmake, Makefile), scripts (.sh), and data files (.csv, .npy) with matrices used in the problems. <100MB Benchmark problem collections Collections of even more optimization and inclusion problems to benchmark the numerical solvers. Reuse existing data Digital Software Plain-text Fortran source code (.f), supporting build recipes (.sh), and problem description files (.sif). <10GB Benchmark results (WP 4) Performance statistics (run time, number of evaluations, suboptimality, constraint violation ...) of the numerical solvers developed as part of the project, as well as the statistics for similar third-party solvers. Generate new data Digital Simulation data Structured records in YAML or JSON format, as well as comma- separated numerical data (CSV). <1GB Benchmark solutions (WP 4) Numerical solutions produced by the different solvers for later comparison, verification, and visualization. Generate new data Digital Simulation data Plain-text or binary numerical data files (.csv, .npy, .mat, .dat). <1GB If you reuse existing data, please specify the source, preferably by using a persistent identifier (e.g. DOI, Handle, URL etc.) per dataset or data type: Software developed as part of previous projects: https://github.com/kul-optec/QPALM https://github.com/kul-optec/QPALM.jl https://github.com/kul-optec/superscs Benchmark problem collections: Gould, N.I.M., Orban, D. & Toint, P.L. CUTEst: a Constrained and Unconstrained Testing Environment with safe threads for mathematical optimization. Comput Optim Appl 60 , 545–557 (2015). https://doi.org/10.1007/s10589-014-9687-3 Are there any ethical issues concerning the creation and/or use of the data (e.g. experiments on humans or animals, dual use)? Describe these issues in the comment section. Please refer to specific datasets or data types when appropriate. No Will you process personal data? If so, briefly describe the kind of personal data you will use in the comment section. Please refer to specific datasets or data types when appropriate. No Does your work have potential for commercial valorization (e.g. tech transfer, for example spin-offs, commercial exploitation, …)? If so, please comment per dataset or data type Created using DMPonline.be. Last modiﬁed 29 April 2023 2 of 5 where appropriate. No Do existing 3rd party agreements restrict exploitation or dissemination of the data you (re)use (e.g. Material/Data transfer agreements/ research collaboration agreements)? If so, please explain in the comment section to what data they relate and what restrictions are in place. No Are there any other legal issues, such as intellectual property rights and ownership, to be managed related to the data you (re)use? If so, please explain in the comment section to what data they relate and which restrictions will be asserted. No 2. Documentation and Metadata Clearly describe what approach will be followed to capture the accompanying information necessary to keep data understandable and usable, for yourself and others, now and in the future (e.g., in terms of documentation levels and types required, procedures used, Electronic Lab Notebooks, README.txt files, Codebook.tsv etc. where this information is recorded). The software is accompanied by extensive documentation, installation instructions and usage examples. The API reference is automatically generated using tools like Doxygen and Sphinx, based on the docstrings and comments present in the source code. Both the documentation and the API reference are available as HTML files and are hosted (either publicly or internally) for easy access. All code is managed using the Git version control system, with clear commit messages, and tags are used to be able to easily switch back to a previous version (e.g. a specific release, or a version corresponding to a publication). The numerical experiments include not only MarkDown or README files describing each experiment and benchmark problem, but also scripts and/or makefiles to easily reproduce the data, as well as scripts to visualize and analyze the existing data. Will a metadata standard be used to make it easier to find and reuse the data? If so, please specify (where appropriate per dataset or data type) which metadata standard will be used. If not, please specify (where appropriate per dataset or data type) which metadata will be created to make the data easier to find and reuse. Yes Benchmark results will be managed using the dman Data Management package to keep structured data about the simulation together with the results. For simulations of specific problems of interest (rather than benchmark collections), a system such as EngMeta could be considered. The software is published on GitHub, with appropriate tags, and made accessible through Zenodo or Papers with Code, which include metadata about the software, and link to the corresponding publications on arXiv. The GitHub repositories also contain a CITATION.cff file to link back to the publications. 3. Data storage & back-up during the research project Where will the data be stored? During active use or development, the data will be stored on the researcher's workstation and/or laptop. Additionally, a time-stamped version of the data will be stored on the STADIUS DATASET Server, which is our research unit's central storage server. Software will not only be kept locally, but also on the KU Leuven GitLab instance (for private projects) and on GitHub (for public/open-source projects) to allow for efficient collaboration. How will the data be backed up? Local copies of the Git repositories are stored on an ESAT workstation which is backed up regularly and automatically. All data on the STADIUS DATASET Server are backed up daily and replicated to an off-site storage system housed in the ICTS data center. Is there currently sufficient storage & backup capacity during the project? If yes, specify concisely. If no or insufficient storage or backup capacities are available, then explain how this will be taken care of. Yes The volume of data is low, storage and backup capacity are not an issue. The STADIUS DATASET Server has a total capacity of 14.88 TB. The capacity of the dataset server is monitored daily by the ESAT system admins. How will you ensure that the data are securely stored and not accessed or modified by unauthorized persons? The private repositories on GitLab are only accessible to authorized users, verified through the central KU Leuven login system, which requires two-factor authentication. The public GitHub repositories are managed by members of the research group only, and require two-factor authentication. Releases to software repositories are automated, and the API tokens necessary to upload new releases are accessible only to the continuous integration workflows that can be triggered by authorized maintainers only. Created using DMPonline.be. Last modiﬁed 29 April 2023 3 of 5 What are the expected costs for data storage and backup during the research project? How will these costs be covered? Data storage and back up will be performed on the STADIUS DATASET Server and the ESAT/KU Leuven GitLab instance in pre-existing storage facilities of the Department of Electrical Engineering (ESAT). The data volume for this project is small (<14GB), and so are the expected costs. For small volumes like this, the costs are covered by the storage provider. 4. Data preservation after the end of the research project Which data will be retained for at least five years (or longer, in agreement with other retention policies that are applicable) after the end of the project? In case some data cannot be preserved, clearly state the reasons for this (e.g. legal or contractual restrictions, storage/budget issues, institutional policies...). All research data (including software and numerical results used in publications) will be preserved for 10 years, according to the KU Leuven RDM policy. Where will these data be archived (stored and curated for the long-term)? The data will be stored on the university's central servers (with automatic back-up procedures). The software itself will be additionally preserved indefinitely in public repositories on GitHub, PyPI and possibly others. What are the expected costs for data preservation during the expected retention period? How will these costs be covered? Free of charge for small datasets. 5. Data sharing and reuse Will the data (or part of the data) be made available for reuse after/during the project? In the comment section please explain per dataset or data type which data will be made available. Yes, in an Open Access repository Yes, in a restricted access repository (after approval, institutional access only, …) Software and numerical results used in publications will be made available in public GitHub repositories. Before publication, development of the software and preliminary experiments will be available in restricted GitLab repositories requiring intitutional access and/or personal authorization. If access is restricted, please specify who will be able to access the data and under what conditions. Restricted GitLab repositories will be accessible to the supervisor and other members of the research group, as well as possible collaborators from other research groups. The access of the data on the STADIUS DATASET Server is regulated by an access control list (ACL) that grants: - read-write access to the project owner and the FWO fellow - read-only access to specific users, such as other members of our research group The ACL is managed by the project owner (Panagiotis Patrinos). Are there any factors that restrict or prevent the sharing of (some of) the data (e.g. as defined in an agreement with a 3rd party, legal restrictions)? Please explain in the comment section per dataset or data type where appropriate. No Where will the data be made available? If already known, please provide a repository per dataset or data type. https://github.com/kul-optec/alpaqa (Software) https://github.com/kul-optec/panoc-gauss-newton-ifac-experiments (Numerical simulations) https://github.com/kul-optec/pantr-cdc2023-experiments (Numerical simulations) When will the data be made available? Upon publication of research results. Which data usage licenses are you going to provide? If none, please explain why. Software will be released under a copyleft license such as the GNU Lesser General Public License. Benchmarks and examples will be made available under a permissive MIT license. Do you intend to add a PID/DOI/accession number to your dataset(s)? If already available, you have the option to provide it in the comment section. Yes Software will be made available through Zenodo or Papers with Code. Created using DMPonline.be. Last modiﬁed 29 April 2023 4 of 5 What are the expected costs for data sharing? How will these costs be covered? GitHub hosting is free for open-source repositories. Sharing code through the KU Leuven GitLab instance is covered by the university. 6. Responsibilities Who will manage data documentation and metadata during the research project? The data documentation and metadata will be managed by the FWO fellow, Pieter Pas. Who will manage data storage and backup during the research project? The FWO fellow, Pieter Pas will be responsible for data storage. The system administrators and data manager of the research division are responsible for the back up during and after the project. Who will manage data preservation and sharing? The FWO Fellow, Pieter Pas, will be responsible for ensuring data preservation and reuse. Who will update and implement this DMP? The project owner (Panagiotis Patrinos) bears the end responsibility of updating and implementing this DMP. Created using DMPonline.be. Last modiﬁed 29 April 2023 5 of 5"
}