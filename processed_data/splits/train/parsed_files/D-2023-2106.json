{
    "document_id": "D-2023-2106",
    "LinkTitle": "D-2023-2106",
    "file_name": "D-2023-2106.pdf",
    "file_path": "/Users/JADEPOTTER5/Downloads/DMP-MT/processed_data/pdfs_new/org_pdfs/D-2023-2106.pdf",
    "metadata": {
        "title": "Data enhanced simulation of wakes for all wind turbines in the North Sea nudged by SCADA data deployed on cloud.",
        "author": "N/A",
        "num_pages": 9
    },
    "content": {
        "full_text": "Data enhanced simulation of wakes for all wind turbines in the North Sea\nnudged by SCADA data deployed on cloud.\nA Data Management Plan created using DMPonline.be\nCreators: \nOlivier Ndindayino, Johan Meyers\nAffiliation: \nKU Leuven (KUL)\nFunder: \nVlaams Agentschap Innoveren & Ondernemen (VLAIO)\nTemplate: \nVLAIO cSBO DMP (Flemish Standard DMP)\nPrincipal Investigator:\n \nJohan Meyers\nData Manager:\n \nOlivier Ndindayino, Johan Meyers\nProject Administrator:\n \nJohan Meyers\nID: \n202042\nStart date: \n01-04-2023\nEnd date: \n01-03-2026\nProject abstract:\nIn large wind farms, the wake effect and blockage effect are underestimated problems. The wind is slowed down during\ninflow, resulting in lower electricity production than expected. \nWind farm design, scenario analysis by governments, grid stability studies, hydrogen wind and energy island design\nneed accurate wake models or digital twin models that are applicable to entire concession zones and make it possible to\nprocess monitoring data. The existing models use hyperparameters to do so.\nCloud4Wake will develop methods to define the hyperparameters on the basis of large data sets and thus optimise the\naccuracy of the models. These data for the North Sea are currently collected from various sources: LIDAR at various\nlocations, meteorological data and data from offshore wind farms.\nIntended project results:\n1\n. \nA new method to estimate wake effects and blockage in a model for a zone of various offshore wind farms. This\nway, losses due to wake effects can be better estimated by the industry; \n2\n. \nA cloud-based framework to calibrate these models on large (field) data sets. It is important that farms collecting\nfield data will use them to optimise the design of their wind farms.\nLast modified: \n09-10-2023\nCreated using DMPonline.be. Last modiﬁed 09 October 2023\n1 of 9\nData enhanced simulation of wakes for all wind turbines in the North Sea\nnudged by SCADA data deployed on cloud.\nVLAIO DMP (Flemish Standard DMP)\n1. Research Data Summary\nList and describe all datasets or research materials that you plan to generate/collect or reuse during your research project. For each dataset or data\ntype (observational, experimental etc.), provide a short name & description (sufficient for yourself to know what data it is about), indicate whether the\ndata are newly generated/collected or reused, digital or physical, also indicate the type of the data (the kind of content), its technical format (file\nextension), and an estimate of the upper limit of the volume of the data.\n \n \n \n \nOnly for digital data\nOnly for digital\ndata \nOnly for digital data \nOnly for\nphysical\ndata\nDataset\nName\nDescription\nNew or reused\nDigital or\nPhysical\nDigital Data Type\nDigital Data\nformat\nDigital data volume\n(MB/GB/TB)\nPhysical\nvolume\n \n \nPlease choose from the following\noptions:\nGenerate new data\nReuse existing data\nPlease choose\nfrom the\nfollowing\noptions:\nDigital\nPhysical\nPlease choose from the\nfollowing options:\nObservational\nExperimental\nCompiled/aggregated\ndata\nSimulation data\nSoftware\nOther\nNA\nPlease choose\nfrom the\nfollowing\noptions:\n.por, .xml,\n.tab,\n.cvs,.pdf,\n.txt, .rtf,\n.dwg,\n.gml, …\nNA\nPlease choose from\nthe following options:\n<100MB\n<1GB\n<100GB\n<1TB\n<5TB\n<10TB\n<50TB\n>50TB\nNA\n \nSimulation\nData\ninput (setup files,\ninitial\nflow fields,\nprecursor data...),\noutput (computed\nvelocity fields and\nprofiles, flow\nstatistics...) and\nmetadata\nproduced by SP-\nWind simulations\nGenerate new data\nDigital\nSimulation data\n.setup (setup\nfiles), .dat (flow\nfields), .txt\n(logs), .exe\n(executables)\n3D flow field\n(input and\noutput, always\ngenerated, e.g.\nBL_field.dat):\n~1-10GB\n(depending on\nthe grid size)\n2D planes\n(output, e.g.\ntime series of\nvelocity\nplanes): ~1-\n100GB\n(depending on\nthe grid\nsize)\nPrecursor\nsimulations:\n~GB-TB\n(depending on\nthe grid size)\nOthers: ~MB\n \nSCADA data\nExperimental data\nfrom windfarms\nReuse existing data\nDigital\nExperimental\nNA\nNA\n \nPost-\nprocessing\nscripts\nPython scripts to\nperform data\nanalysis on the\nsimulation data\nGenerate new data and Reuse existing\ndata\nDigital\nsoftware\n.py (Python)\n~100MB\n \nPublications\ndata related to\npublication papers\nGenerate new data\nDigital\ntext and figures\n.pdf (final pdf),\n.tex (LaTex\nfiles), .png/.jpg\n(figures), .doc\n(word\ndocument)\n~1GB\n \nPresentations\nmeeting and\nconference\npresentations\nGenerate new data\nDigital\npresentations and figures\n.ppt\n(PowerPoint),\n.pdf, .jpg/.png\n(figures), .doc\n(word\ndocument), .tex\n(LaTex files)\n~1GB\n \nLiterature\nbooks and papers\nReuse existing data\nDigital and\nphysical\ntextual data, papers, books\n.pdf\n~1GB\n~100 books\n(in the\ndepartment)\n \nIf you reuse existing data, please specify the source, preferably by using a persistent identifier (e.g. DOI, Handle, URL etc.) per dataset or data type:\nSimulation Data :\nCreated using DMPonline.be. Last modiﬁed 09 October 2023\n2 of 9\nInput data (the case setup) is case specific and generated by the user,\ntemplates are included in the SP-Wind code repository. Output data and metadata are generated by the SP-Wind simulation code.\n \nSCADA data :\nOriginating from wind farm owners and operators\n \nPost-processing scripts :\nPy4sp python code developed within the TSFO group accessible on the KU Leuven gitlab\n \nPublications :\nFigures obtained from analysis of simulation data through post-processing scripts\n \nPresentations :\nOwn presentations and figures obtained from analysis of simulation data through post-processing scripts\n \nLiterature :\nOnline academic sites: Limo (https://limo.libis.be/index.html) google scholar (https://scholar.google.com/), library,\n(physical) books from the TME department\nAre there any ethical issues concerning the creation and/or use of the data (e.g. experiments on humans or animals, dual use)? Describe these issues\nin the comment section. Please refer to specific datasets or data types when appropriate.\nNo\nWill you process personal data? If so, briefly describe the kind of personal data you will use in the comment section. Please refer to specific datasets or\ndata types when appropriate.\nNo\nDoes your work have potential for commercial valorization (e.g. tech transfer, for example spin-offs, commercial exploitation, …)? If so, please\ncomment per dataset or data type where appropriate.\nNo\nDo existing 3rd party agreements restrict exploitation or dissemination of the data you (re)use (e.g. Material/Data transfer agreements/ research\ncollaboration agreements)? If so, please explain in the comment section to what data they relate and what restrictions are in place.\nYes\n \nYes, Intellectual property rights on SCADA data and other data coming from wind farm owners and operators.\nAre there any other legal issues, such as intellectual property rights and ownership, to be managed related to the data you (re)use? If so, please\nexplain in the comment section to what data they relate and which restrictions will be asserted.\nNo\n2. Documentation and Metadata\nClearly describe what approach will be followed to capture the accompanying information necessary to keep data understandable and usable, for\nyourself and others, now and in the future (e.g., in terms of documentation levels and types required, procedures used, Electronic Lab Notebooks,\nREADME.txt files, Codebook.tsv etc. where this information is recorded).\nSimulation Data :\nEach simulation is assigned a distinctive code, which are linked to an individual folder identified by the same code. This folder contains the necessary files for the\nsimulation, including case setup files, source code or executable (for replication), and the output data.\nIn order to monitor and document the simulations, an Excel spreadsheet \"Simlist.xlsx\" is used to record all simulation details. This spreadsheet serves as a repository for\nmetadata related to each simulation, organized in accordance with the Dublin Core general metadata standard (outlined below). Additionally, within each simulation\nfolder, there is a README file that provides metadata specific to the simulation. This README is automatically generated, through python scripts, from the case setup files\nand is further manually updated with metadata according to the Dublin Core metadata standard. The Simlist.xlsx logbook is generated automatically, through python\nCreated using DMPonline.be. Last modiﬁed 09 October 2023\n3 of 9\nscripts, based on the case setup files and README.\nThis simulation logging system was put in place by a PhD student within the department and is accessible on the KU Leuven GitLab server.\n \nPost-processing scripts :\nThe Python scripts feature a header that provides an overview of the script's purpose, accompanied by explanatory comments interspersed within the code.\n \nPublications\nMetadata is included in the publications.\n \nPresentations\nMetadata is included in the presentations.\n \nLiterature\nPapers are organized by topic, using the Mendeley reference manager.\n \nWill a metadata standard be used to make it easier to find and reuse the data? If so, please specify (where appropriate per dataset or data type) which\nmetadata standard will be used. If not, please specify (where appropriate per dataset or data type) which metadata will be created to make the data\neasier to find and reuse.\nYes\nMetadata for the performed simulations is organized according to the Dublin Core\nmetadata standard, adapted for our purposes.\nApplied to our simulations, the README in the simulation folder (containing the modified\nDublin Core metadata) has the following entries/sections:\nGeneral information:\n cf. Dublin Core (to be filled in manually, this is the responsibility of the person running the simulations)\nProject: project name\nDescription: description\nPurpose: the purpose of the simulation\nInitialized from: relation with previous simulations\nRelevant output: list of (relevant) simulation output files and their file format\nFindings: (brief) discussion of the most important results\nAuthor: person who runs the simulation\nDate submitted: date the simulation was submitted\nHorizon: recommendations on how long the data should be stored\nCase setup\n: metadata on the case setup, e.g. grid resolution, boundary conditions... (automated)\nGit info\n: git branch and commit hash of the code used in the simulation (automated,\nimportant for reproduction)\nVSC information\n: start time and end time of the simulation, summary of used resources\n(from the Vlaams Supercomputer Center)\nAll this metadata is also included in the logbook Simlist.xlsx that contains an overview of all\nsimulations. The logbook also stores the location of the simulation folder.\n3. Data storage & back-up during the research project\nWhere will the data be stored?\nThe Vlaams Supercomputer Centrum (VSC) provides a high-performance computing (HPC) supercomputing platform for the simulations. Additionally, they offer\ncomplimentary storage infrastructure on their clusters:\n$VSC_DATA: hourly backup, 75 GB, permanent storage\n$VSC_SCRATCH: 5TB, data deleted 28 days from last access\n$VSC_SCRATCH_NODE: 200GB/node available only at runtime (while running a simulation)\nStaging (/staging/leuven): permanent storage, the research group uses approx 120TB\nshared among all researchers within the group, no backup.\n \nSimulation Data\nSimulations run on the HPC cluster, in their scratch folder on\n$VSC_SCRATCH. Runtime data generated during the simulations (e.g. intermediary flow\nfields, ~GB-TB) are stored in the scratch folder ($VSC_SCRATCH), if <~4TB. Scratch storage is limited, in volume (<5TB) and time (28 days after last access). Therefore,\nwhen simulations are finished:\n1. The folder is cleaned: unimportant data (data that will never be reused or data that can be easily reproduced) is removed.\n2. The cleaned folder is stored on the TFSO staging folder (/staging/leuven) on the HPC cluster. This is the case for important simulations (important results, validations...).\nOr it is stored on $VSC_DATA. This is the case if the simulations do not contain important results,\nare limited in volume and/or will be quickly reused. \nThe metadata of the simulations is stored within the README file located in the simulation folder, as well as in the comprehensive Excel spreadsheet, Simlist.xlsx. This\nspreadsheet is stored in $VSC_DATA, with an additional copy stored on the PC. Significant results and graphical representations are summarized in Powerpoint or LaTex\nfiles in a dedicated documentation folder on the PC.\n \nPostprocessing scripts\nPostprocessing scripts are stored on the HPC cluster, in $VSC_DATA. In that way, postprocessing can happen on the HPC cluster, such that there is no need to copy\nCreated using DMPonline.be. Last modiﬁed 09 October 2023\n4 of 9\n(potentially very large) data sets to the PC.\n \nPublications\nData concerning publications are stored in a personal KU Leuven OneDrive folder.\n \nPresentations\nPresentations are stored in a personal KU Leuven OneDrive folder.\n \nLiterature\n(Digital) papers and books are stored in a personal KU Leuven OneDrive folder, and they are managed via Mendeley Reference Manager.\nHow will the data be backed up?\nSeveral storage options outlined above provide automatic backup as a service:\nThe VSC provides (hourly) backups for data in $VSC_DATA. Data stored on remote git servers is backed up by Git itself. KU Leuven Onedrive folder includes version\nhistory.\nNote that staging and scratch on the HPC cluster are not back-upped. Since scratch storage is temporary, important data there is always\nimmediately copied elsewhere (as outlined above).\nIs there currently sufficient storage & backup capacity during the project? If yes, specify concisely.\nIf no or insufficient storage or backup capacities are available, then explain how this will be taken care of.\nNo\nStaging storage on the HPC cluster (/staging/leuven) may not suffice, since it is shared\namong all researchers of the TFSO group. In the case of insufficient staging:\nFirst, MANGO will be used (this is a KU Leuven storage system for active research data).\nSecondly, the staging and archiving folders will be cleaned (individually by all TFSO\nresearchers), removing unimportant/redundant files. The simulation logbook Simlist.xlsx provides a useful utility to organize file removal in a structured manner. The\nlogbook contains metadata describing how long each simulation data should be kept (so by scrolling\nthrough the logbook, researchers can find simulations past their storage horizon which can\ntherefore be safely removed), as well as the amount of resources the simulation\nconsumed (so by scrolling through the logbook, researchers can find simulation data that can be removed since the simulation is easily reproducible).\nHow will you ensure that the data are securely stored and not accessed or modified by unauthorized persons?\nRegarding the HPC infrastructure at the HPC:\nAll data in $VSC_DATA and $VSC_SCRATCH are only accessible by the the user of the VSC-account. It is password protected via a personal private key (public/private key\npairs were generated in order to get access to the VSC)\nStaging data is accessible for all TFSO group members, via their own VSC account\n(that is also password protected, public/private key infrastructure)\nData stored on personal laptop and personal OneDrive is only accessible by the user (password protected).\nWhat are the expected costs for data storage and backup during the research project? How will these costs be covered?\nBasic storage and backup infrastructure (as well as scratch extension) at VSC is costless\n(75GB $VSC_DATA, 5TB $VSC_SCRATCH, 200GB/node $VSC_SCRATCH_NODE).\nStaging cost at HPC (/staging/leuven): €20/TB/year * 120TB = €2400/year (for the whole\nTFSO research group). Cost is carried by the TFSO group.\nGitLab, KU Leuven OneDrive, storage on personal laptop are costless.\nThere are possible costs related to larger data sets stored on MANGO and KU Leuven RDR (~30 euro/Tb/year).\n4. Data preservation after the end of the research project\nWhich data will be retained for at least five years (or longer, in agreement with other retention policies that are applicable) after the end of the\nproject? In case some data cannot be preserved, clearly state the reasons for this (e.g. legal or contractual restrictions, storage/budget issues,\ninstitutional policies...).\nOnly data related to publications or important benchmark/validation data will be preserved. The code generated during the PhD is also preserved in the\ncorresponding git repositories. This data will be kept for a period of minimally 10 years after the end of the project.\n \nPublications\nAll data related to publications will be preserved.\n \nPresentations\nRelevant presentation are preserved: conference presentations, presentations serving as documentation for major code developments.\nOther presentations - e.g. for TFSO group meetings, weekly meetings with promotor... - are not preserved.\n \nCreated using DMPonline.be. Last modiﬁed 09 October 2023\n5 of 9\nLiterature\nNot preserved (except for the physical material that is kept at the department).\nWhere will these data be archived (stored and curated for the long-term)?\nSimulation Data\nAfter the project, the simulation data in the staging folder is examined:\nFirst, research data that can easily be reproduced will be deleted. Clear\ninstructions on how to reproduce the simulations (e.g.: which code to use) based on the README in the simulation folder, will be provided.\nA selection of the data that will be kept in the staging folder is made. This data, easily\naccessible on the HPC cluster, may be used in the future by other researchers in the\nTFSO group. This includes benchmark simulations and important research data (that\ncan serve as a mean to validate future research, serve as comparison...), but also\nsimulation data that took a very long time to compute such as extensive precursor\nsimulations (as this will facilitate future research).\nWhat are the expected costs for data preservation during the expected retention period? How will these costs be covered?\nStaging cost at HPC (/staging/leuven): €20/TB/year * 120TB = €2400/year (for the whole\nTFSO research group). The cost is carried by the TFSO group.\nGitLab repositories are costless.\nThere are possible costs related to larger data sets stored on MANGO and KU Leuven RDR (~30 euro/Tb/year).\n5. Data sharing and reuse\nWill the data (or part of the data) be made available for reuse after/during the project?  In the comment section please explain per dataset or data type\nwhich data will be made available.\nNo (closed access)\nYes, in an Open Access repository\nYes, in a restricted access repository (after approval, institutional access only, …)\nIf access is restricted, please specify who will be able to access the data and under what conditions.\nOnly KU Leuven personel on a need to know basis and compliant with relevant NDAs.\nAre there any factors that restrict or prevent the sharing of (some of) the data (e.g. as defined in an agreement with a 3rd party, legal restrictions)?\nPlease explain in the comment section per dataset or data type where appropriate.\nYes, Intellectual Property Rights\nSCADA data and other data coming from wind farm owners and operators.\n \nWhere will the data be made available? If already known, please provide a repository per dataset or data type.\nIn an Open Access repository\nIn a restricted access repository\nUpon request by mail\nSimulation Data\nRelevant simulation data is accessible for TFSO researchers via the staging folder of the HPC cluster and MANGO data repository.\n \nPostprocessing scripts\nThe postprocessing scripts will be integrated in a general postprocessing framework for all SP-Wind users, stored on a git repository. The scripts are accessible from there\n(for future TFSOresearchers).\n \nPublications\nAll data related to publications (as explicited in the previous sections) will beaccessible. The manuscripts of the publications is also accessible via Lirias (open\naccess). For as far as this is possible for data size all scripts and data will be shared through KU Leuven RDR.\n \nWhen will the data be made available?\nCreated using DMPonline.be. Last modiﬁed 09 October 2023\n6 of 9\nAt the time of publication.\nAll data that will be shared will be available immediately (as soon as they are stored on the\narchive folder of the HPC cluster, the shared folder, git...).\nWhich data usage licenses are you going to provide? If none, please explain why.\nGNU  license with no commercial use or freer\nDo you intend to add a PID/DOI/accession number to your dataset(s)? If already available, you have the option to provide it in the comment section.\nYes\nWhen published on KU Leuven RDR.\nWhat are the expected costs for data sharing? How will these costs be covered?\nThere are possible costs related to larger data sets stored on MANGO and KU Leuven RDR (~30 euro/Tb/year).\n6. Responsibilities\nWho will manage data documentation and metadata during the research project?\nOlivier Ndindayino and Johan Meyers \nWho will manage data storage and backup during the research project?\nOlivier Ndindayino and Johan Meyers \nWho will manage data preservation and sharing?\nJohan Meyers \nWho will update and implement this DMP?\nOlivier Ndindayino and Johan Meyers \nCreated using DMPonline.be. Last modiﬁed 09 October 2023\n7 of 9\nData enhanced simulation of wakes for all wind turbines in the North Sea\nnudged by SCADA data deployed on cloud.\nGDPR\nGDPR\nHave you registered personal data processing activities for this project?\nNo\nCreated using DMPonline.be. Last modiﬁed 09 October 2023\n8 of 9\nData enhanced simulation of wakes for all wind turbines in the North Sea\nnudged by SCADA data deployed on cloud.\nDPIA\nDPIA\nHave you performed a DPIA for the personal data processing activities for this project?\nNot applicable\nCreated using DMPonline.be. Last modiﬁed 09 October 2023\n9 of 9"
    },
    "clean_full_text": "Data enhanced simulation of wakes for all wind turbines in the North Sea nudged by SCADA data deployed on cloud. A Data Management Plan created using DMPonline.be Creators: Olivier Ndindayino, Johan Meyers Affiliation: KU Leuven (KUL) Funder: Vlaams Agentschap Innoveren & Ondernemen (VLAIO) Template: VLAIO cSBO DMP (Flemish Standard DMP) Principal Investigator: Johan Meyers Data Manager: Olivier Ndindayino, Johan Meyers Project Administrator: Johan Meyers ID: 202042 Start date: 01-04-2023 End date: 01-03-2026 Project abstract: In large wind farms, the wake effect and blockage effect are underestimated problems. The wind is slowed down during inflow, resulting in lower electricity production than expected. Wind farm design, scenario analysis by governments, grid stability studies, hydrogen wind and energy island design need accurate wake models or digital twin models that are applicable to entire concession zones and make it possible to process monitoring data. The existing models use hyperparameters to do so. Cloud4Wake will develop methods to define the hyperparameters on the basis of large data sets and thus optimise the accuracy of the models. These data for the North Sea are currently collected from various sources: LIDAR at various locations, meteorological data and data from offshore wind farms. Intended project results: 1 . A new method to estimate wake effects and blockage in a model for a zone of various offshore wind farms. This way, losses due to wake effects can be better estimated by the industry; 2 . A cloud-based framework to calibrate these models on large (field) data sets. It is important that farms collecting field data will use them to optimise the design of their wind farms. Last modified: 09-10-2023 Created using DMPonline.be. Last modiﬁed 09 October 2023 1 of 9 Data enhanced simulation of wakes for all wind turbines in the North Sea nudged by SCADA data deployed on cloud. VLAIO DMP (Flemish Standard DMP) 1. Research Data Summary List and describe all datasets or research materials that you plan to generate/collect or reuse during your research project. For each dataset or data type (observational, experimental etc.), provide a short name & description (sufficient for yourself to know what data it is about), indicate whether the data are newly generated/collected or reused, digital or physical, also indicate the type of the data (the kind of content), its technical format (file extension), and an estimate of the upper limit of the volume of the data. Only for digital data Only for digital data Only for digital data Only for physical data Dataset Name Description New or reused Digital or Physical Digital Data Type Digital Data format Digital data volume (MB/GB/TB) Physical volume Please choose from the following options: Generate new data Reuse existing data Please choose from the following options: Digital Physical Please choose from the following options: Observational Experimental Compiled/aggregated data Simulation data Software Other NA Please choose from the following options: .por, .xml, .tab, .cvs,.pdf, .txt, .rtf, .dwg, .gml, … NA Please choose from the following options: <100MB <1GB <100GB <1TB <5TB <10TB <50TB >50TB NA Simulation Data input (setup files, initial flow fields, precursor data...), output (computed velocity fields and profiles, flow statistics...) and metadata produced by SP- Wind simulations Generate new data Digital Simulation data .setup (setup files), .dat (flow fields), .txt (logs), .exe (executables) 3D flow field (input and output, always generated, e.g. BL_field.dat): ~1-10GB (depending on the grid size) 2D planes (output, e.g. time series of velocity planes): ~1- 100GB (depending on the grid size) Precursor simulations: ~GB-TB (depending on the grid size) Others: ~MB SCADA data Experimental data from windfarms Reuse existing data Digital Experimental NA NA Post- processing scripts Python scripts to perform data analysis on the simulation data Generate new data and Reuse existing data Digital software .py (Python) ~100MB Publications data related to publication papers Generate new data Digital text and figures .pdf (final pdf), .tex (LaTex files), .png/.jpg (figures), .doc (word document) ~1GB Presentations meeting and conference presentations Generate new data Digital presentations and figures .ppt (PowerPoint), .pdf, .jpg/.png (figures), .doc (word document), .tex (LaTex files) ~1GB Literature books and papers Reuse existing data Digital and physical textual data, papers, books .pdf ~1GB ~100 books (in the department) If you reuse existing data, please specify the source, preferably by using a persistent identifier (e.g. DOI, Handle, URL etc.) per dataset or data type: Simulation Data : Created using DMPonline.be. Last modiﬁed 09 October 2023 2 of 9 Input data (the case setup) is case specific and generated by the user, templates are included in the SP-Wind code repository. Output data and metadata are generated by the SP-Wind simulation code. SCADA data : Originating from wind farm owners and operators Post-processing scripts : Py4sp python code developed within the TSFO group accessible on the KU Leuven gitlab Publications : Figures obtained from analysis of simulation data through post-processing scripts Presentations : Own presentations and figures obtained from analysis of simulation data through post-processing scripts Literature : Online academic sites: Limo (https://limo.libis.be/index.html) google scholar (https://scholar.google.com/), library, (physical) books from the TME department Are there any ethical issues concerning the creation and/or use of the data (e.g. experiments on humans or animals, dual use)? Describe these issues in the comment section. Please refer to specific datasets or data types when appropriate. No Will you process personal data? If so, briefly describe the kind of personal data you will use in the comment section. Please refer to specific datasets or data types when appropriate. No Does your work have potential for commercial valorization (e.g. tech transfer, for example spin-offs, commercial exploitation, …)? If so, please comment per dataset or data type where appropriate. No Do existing 3rd party agreements restrict exploitation or dissemination of the data you (re)use (e.g. Material/Data transfer agreements/ research collaboration agreements)? If so, please explain in the comment section to what data they relate and what restrictions are in place. Yes Yes, Intellectual property rights on SCADA data and other data coming from wind farm owners and operators. Are there any other legal issues, such as intellectual property rights and ownership, to be managed related to the data you (re)use? If so, please explain in the comment section to what data they relate and which restrictions will be asserted. No 2. Documentation and Metadata Clearly describe what approach will be followed to capture the accompanying information necessary to keep data understandable and usable, for yourself and others, now and in the future (e.g., in terms of documentation levels and types required, procedures used, Electronic Lab Notebooks, README.txt files, Codebook.tsv etc. where this information is recorded). Simulation Data : Each simulation is assigned a distinctive code, which are linked to an individual folder identified by the same code. This folder contains the necessary files for the simulation, including case setup files, source code or executable (for replication), and the output data. In order to monitor and document the simulations, an Excel spreadsheet \"Simlist.xlsx\" is used to record all simulation details. This spreadsheet serves as a repository for metadata related to each simulation, organized in accordance with the Dublin Core general metadata standard (outlined below). Additionally, within each simulation folder, there is a README file that provides metadata specific to the simulation. This README is automatically generated, through python scripts, from the case setup files and is further manually updated with metadata according to the Dublin Core metadata standard. The Simlist.xlsx logbook is generated automatically, through python Created using DMPonline.be. Last modiﬁed 09 October 2023 3 of 9 scripts, based on the case setup files and README. This simulation logging system was put in place by a PhD student within the department and is accessible on the KU Leuven GitLab server. Post-processing scripts : The Python scripts feature a header that provides an overview of the script's purpose, accompanied by explanatory comments interspersed within the code. Publications Metadata is included in the publications. Presentations Metadata is included in the presentations. Literature Papers are organized by topic, using the Mendeley reference manager. Will a metadata standard be used to make it easier to find and reuse the data? If so, please specify (where appropriate per dataset or data type) which metadata standard will be used. If not, please specify (where appropriate per dataset or data type) which metadata will be created to make the data easier to find and reuse. Yes Metadata for the performed simulations is organized according to the Dublin Core metadata standard, adapted for our purposes. Applied to our simulations, the README in the simulation folder (containing the modified Dublin Core metadata) has the following entries/sections: General information: cf. Dublin Core (to be filled in manually, this is the responsibility of the person running the simulations) Project: project name Description: description Purpose: the purpose of the simulation Initialized from: relation with previous simulations Relevant output: list of (relevant) simulation output files and their file format Findings: (brief) discussion of the most important results Author: person who runs the simulation Date submitted: date the simulation was submitted Horizon: recommendations on how long the data should be stored Case setup : metadata on the case setup, e.g. grid resolution, boundary conditions... (automated) Git info : git branch and commit hash of the code used in the simulation (automated, important for reproduction) VSC information : start time and end time of the simulation, summary of used resources (from the Vlaams Supercomputer Center) All this metadata is also included in the logbook Simlist.xlsx that contains an overview of all simulations. The logbook also stores the location of the simulation folder. 3. Data storage & back-up during the research project Where will the data be stored? The Vlaams Supercomputer Centrum (VSC) provides a high-performance computing (HPC) supercomputing platform for the simulations. Additionally, they offer complimentary storage infrastructure on their clusters: $VSC_DATA: hourly backup, 75 GB, permanent storage $VSC_SCRATCH: 5TB, data deleted 28 days from last access $VSC_SCRATCH_NODE: 200GB/node available only at runtime (while running a simulation) Staging (/staging/leuven): permanent storage, the research group uses approx 120TB shared among all researchers within the group, no backup. Simulation Data Simulations run on the HPC cluster, in their scratch folder on $VSC_SCRATCH. Runtime data generated during the simulations (e.g. intermediary flow fields, ~GB-TB) are stored in the scratch folder ($VSC_SCRATCH), if <~4TB. Scratch storage is limited, in volume (<5TB) and time (28 days after last access). Therefore, when simulations are finished: 1. The folder is cleaned: unimportant data (data that will never be reused or data that can be easily reproduced) is removed. 2. The cleaned folder is stored on the TFSO staging folder (/staging/leuven) on the HPC cluster. This is the case for important simulations (important results, validations...). Or it is stored on $VSC_DATA. This is the case if the simulations do not contain important results, are limited in volume and/or will be quickly reused. The metadata of the simulations is stored within the README file located in the simulation folder, as well as in the comprehensive Excel spreadsheet, Simlist.xlsx. This spreadsheet is stored in $VSC_DATA, with an additional copy stored on the PC. Significant results and graphical representations are summarized in Powerpoint or LaTex files in a dedicated documentation folder on the PC. Postprocessing scripts Postprocessing scripts are stored on the HPC cluster, in $VSC_DATA. In that way, postprocessing can happen on the HPC cluster, such that there is no need to copy Created using DMPonline.be. Last modiﬁed 09 October 2023 4 of 9 (potentially very large) data sets to the PC. Publications Data concerning publications are stored in a personal KU Leuven OneDrive folder. Presentations Presentations are stored in a personal KU Leuven OneDrive folder. Literature (Digital) papers and books are stored in a personal KU Leuven OneDrive folder, and they are managed via Mendeley Reference Manager. How will the data be backed up? Several storage options outlined above provide automatic backup as a service: The VSC provides (hourly) backups for data in $VSC_DATA. Data stored on remote git servers is backed up by Git itself. KU Leuven Onedrive folder includes version history. Note that staging and scratch on the HPC cluster are not back-upped. Since scratch storage is temporary, important data there is always immediately copied elsewhere (as outlined above). Is there currently sufficient storage & backup capacity during the project? If yes, specify concisely. If no or insufficient storage or backup capacities are available, then explain how this will be taken care of. No Staging storage on the HPC cluster (/staging/leuven) may not suffice, since it is shared among all researchers of the TFSO group. In the case of insufficient staging: First, MANGO will be used (this is a KU Leuven storage system for active research data). Secondly, the staging and archiving folders will be cleaned (individually by all TFSO researchers), removing unimportant/redundant files. The simulation logbook Simlist.xlsx provides a useful utility to organize file removal in a structured manner. The logbook contains metadata describing how long each simulation data should be kept (so by scrolling through the logbook, researchers can find simulations past their storage horizon which can therefore be safely removed), as well as the amount of resources the simulation consumed (so by scrolling through the logbook, researchers can find simulation data that can be removed since the simulation is easily reproducible). How will you ensure that the data are securely stored and not accessed or modified by unauthorized persons? Regarding the HPC infrastructure at the HPC: All data in $VSC_DATA and $VSC_SCRATCH are only accessible by the the user of the VSC-account. It is password protected via a personal private key (public/private key pairs were generated in order to get access to the VSC) Staging data is accessible for all TFSO group members, via their own VSC account (that is also password protected, public/private key infrastructure) Data stored on personal laptop and personal OneDrive is only accessible by the user (password protected). What are the expected costs for data storage and backup during the research project? How will these costs be covered? Basic storage and backup infrastructure (as well as scratch extension) at VSC is costless (75GB $VSC_DATA, 5TB $VSC_SCRATCH, 200GB/node $VSC_SCRATCH_NODE). Staging cost at HPC (/staging/leuven): €20/TB/year * 120TB = €2400/year (for the whole TFSO research group). Cost is carried by the TFSO group. GitLab, KU Leuven OneDrive, storage on personal laptop are costless. There are possible costs related to larger data sets stored on MANGO and KU Leuven RDR (~30 euro/Tb/year). 4. Data preservation after the end of the research project Which data will be retained for at least five years (or longer, in agreement with other retention policies that are applicable) after the end of the project? In case some data cannot be preserved, clearly state the reasons for this (e.g. legal or contractual restrictions, storage/budget issues, institutional policies...). Only data related to publications or important benchmark/validation data will be preserved. The code generated during the PhD is also preserved in the corresponding git repositories. This data will be kept for a period of minimally 10 years after the end of the project. Publications All data related to publications will be preserved. Presentations Relevant presentation are preserved: conference presentations, presentations serving as documentation for major code developments. Other presentations - e.g. for TFSO group meetings, weekly meetings with promotor... - are not preserved. Created using DMPonline.be. Last modiﬁed 09 October 2023 5 of 9 Literature Not preserved (except for the physical material that is kept at the department). Where will these data be archived (stored and curated for the long-term)? Simulation Data After the project, the simulation data in the staging folder is examined: First, research data that can easily be reproduced will be deleted. Clear instructions on how to reproduce the simulations (e.g.: which code to use) based on the README in the simulation folder, will be provided. A selection of the data that will be kept in the staging folder is made. This data, easily accessible on the HPC cluster, may be used in the future by other researchers in the TFSO group. This includes benchmark simulations and important research data (that can serve as a mean to validate future research, serve as comparison...), but also simulation data that took a very long time to compute such as extensive precursor simulations (as this will facilitate future research). What are the expected costs for data preservation during the expected retention period? How will these costs be covered? Staging cost at HPC (/staging/leuven): €20/TB/year * 120TB = €2400/year (for the whole TFSO research group). The cost is carried by the TFSO group. GitLab repositories are costless. There are possible costs related to larger data sets stored on MANGO and KU Leuven RDR (~30 euro/Tb/year). 5. Data sharing and reuse Will the data (or part of the data) be made available for reuse after/during the project? In the comment section please explain per dataset or data type which data will be made available. No (closed access) Yes, in an Open Access repository Yes, in a restricted access repository (after approval, institutional access only, …) If access is restricted, please specify who will be able to access the data and under what conditions. Only KU Leuven personel on a need to know basis and compliant with relevant NDAs. Are there any factors that restrict or prevent the sharing of (some of) the data (e.g. as defined in an agreement with a 3rd party, legal restrictions)? Please explain in the comment section per dataset or data type where appropriate. Yes, Intellectual Property Rights SCADA data and other data coming from wind farm owners and operators. Where will the data be made available? If already known, please provide a repository per dataset or data type. In an Open Access repository In a restricted access repository Upon request by mail Simulation Data Relevant simulation data is accessible for TFSO researchers via the staging folder of the HPC cluster and MANGO data repository. Postprocessing scripts The postprocessing scripts will be integrated in a general postprocessing framework for all SP-Wind users, stored on a git repository. The scripts are accessible from there (for future TFSOresearchers). Publications All data related to publications (as explicited in the previous sections) will beaccessible. The manuscripts of the publications is also accessible via Lirias (open access). For as far as this is possible for data size all scripts and data will be shared through KU Leuven RDR. When will the data be made available? Created using DMPonline.be. Last modiﬁed 09 October 2023 6 of 9 At the time of publication. All data that will be shared will be available immediately (as soon as they are stored on the archive folder of the HPC cluster, the shared folder, git...). Which data usage licenses are you going to provide? If none, please explain why. GNU license with no commercial use or freer Do you intend to add a PID/DOI/accession number to your dataset(s)? If already available, you have the option to provide it in the comment section. Yes When published on KU Leuven RDR. What are the expected costs for data sharing? How will these costs be covered? There are possible costs related to larger data sets stored on MANGO and KU Leuven RDR (~30 euro/Tb/year). 6. Responsibilities Who will manage data documentation and metadata during the research project? Olivier Ndindayino and Johan Meyers Who will manage data storage and backup during the research project? Olivier Ndindayino and Johan Meyers Who will manage data preservation and sharing? Johan Meyers Who will update and implement this DMP? Olivier Ndindayino and Johan Meyers Created using DMPonline.be. Last modiﬁed 09 October 2023 7 of 9 Data enhanced simulation of wakes for all wind turbines in the North Sea nudged by SCADA data deployed on cloud. GDPR GDPR Have you registered personal data processing activities for this project? No Created using DMPonline.be. Last modiﬁed 09 October 2023 8 of 9 Data enhanced simulation of wakes for all wind turbines in the North Sea nudged by SCADA data deployed on cloud. DPIA DPIA Have you performed a DPIA for the personal data processing activities for this project? Not applicable Created using DMPonline.be. Last modiﬁed 09 October 2023 9 of 9"
}